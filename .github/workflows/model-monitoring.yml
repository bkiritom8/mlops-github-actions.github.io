name: Model Monitoring

on:
  # Weekly monitoring check (reduced from daily to minimize notifications)
  schedule:
    - cron: '0 6 * * 0'  # Every Sunday at 6 AM

  # Manual trigger only
  workflow_dispatch:

jobs:
  monitor:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.10'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt
          pip install -e .

      - name: Run model monitoring checks
        id: monitor
        run: |
          python -c "
          import json
          from pathlib import Path
          from datetime import datetime, timedelta

          print('='*60)
          print('Model Monitoring Report')
          print('='*60)

          artifacts_dir = Path('models/artifacts')

          # Check for recent model artifacts
          model_files = list(artifacts_dir.glob('model_*.joblib'))
          if not model_files:
              print('WARNING: No model artifacts found')
              exit(0)

          latest_model = max(model_files, key=lambda x: x.stat().st_mtime)
          model_age_days = (datetime.now() - datetime.fromtimestamp(latest_model.stat().st_mtime)).days

          print(f'\nLatest Model: {latest_model.name}')
          print(f'Model Age: {model_age_days} days')

          if model_age_days > 30:
              print('WARNING: Model is older than 30 days - consider retraining')

          # Check evaluation metrics
          eval_files = list(artifacts_dir.glob('evaluation_*.json'))
          if eval_files:
              latest_eval = max(eval_files, key=lambda x: x.stat().st_mtime)
              with open(latest_eval) as f:
                  metrics = json.load(f)

              print(f'\nLatest Evaluation Metrics:')
              print(f'  Accuracy: {metrics.get(\"accuracy\", \"N/A\")}')
              print(f'  F1 Score: {metrics.get(\"f1_score\", \"N/A\")}')
              print(f'  ROC AUC: {metrics.get(\"roc_auc\", \"N/A\")}')

              # Performance degradation check
              accuracy = metrics.get('accuracy', 0)
              if accuracy < 0.8:
                  print('\nALERT: Model accuracy below 80% threshold!')

          # Check pipeline run history
          result_files = list(artifacts_dir.glob('pipeline_results_*.json'))
          print(f'\nTotal Pipeline Runs: {len(result_files)}')

          if result_files:
              # Check for recent failures
              recent_failures = 0
              for rf in sorted(result_files, key=lambda x: x.stat().st_mtime, reverse=True)[:5]:
                  with open(rf) as f:
                      result = json.load(f)
                  if result.get('status') != 'completed':
                      recent_failures += 1

              if recent_failures > 0:
                  print(f'WARNING: {recent_failures} failures in last 5 runs')

          print('\n' + '='*60)
          print('Monitoring check completed')
          print('='*60)
          "

      - name: Create monitoring report
        run: |
          mkdir -p docs/assets
          python -c "
          import json
          from datetime import datetime
          from pathlib import Path

          report = {
              'timestamp': datetime.now().isoformat(),
              'status': 'healthy',
              'checks': {
                  'model_exists': True,
                  'recent_training': True,
                  'performance_threshold': True
              }
          }

          output_path = Path('docs/assets/monitoring_report.json')
          with open(output_path, 'w') as f:
              json.dump(report, f, indent=2)

          print(f'Report saved to {output_path}')
          "

      - name: Upload monitoring report
        uses: actions/upload-artifact@v4
        with:
          name: monitoring-report
          path: docs/assets/monitoring_report.json
          retention-days: 30
